# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vJrFkq1l8aExf6-P1fcCPo8v4BN54FeN
"""

# prompt: mount google drive

from google.colab import drive
drive.mount('/content/drive')

import os
import gdown
import zipfile
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image, ImageFilter
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import OxfordIIITPet
from torchvision.transforms import functional as TF
from torchvision.utils import save_image
from skimage.metrics import structural_similarity as compare_ssim


class BlurAndDownsample(nn.Module):
    def __init__(self, sigma=1.0, scale_factor=2):
        super(BlurAndDownsample, self).__init__()
        self.sigma = sigma
        self.scale_factor = scale_factor

    def gaussian_blur(self, img):
        # Applying Gaussian blur
        img = img.filter(ImageFilter.GaussianBlur(self.sigma))
        return img

    def downsample(self, img):
        # Downsample using bicubic interpolation
        width, height = img.size
        new_size = (width // self.scale_factor, height // self.scale_factor)
        img = TF.resize(img, new_size, interpolation=transforms.InterpolationMode.BICUBIC)
        return img

    def forward(self, img):
        img = self.gaussian_blur(img)
        img = self.downsample(img)
        return img


def prepare_datasets(image_size=128, batch_size=32, scale_factor=2, test_split=0.2):
    # High-Resolution Transform
    hr_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Low-Resolution Transform
    lr_transform = transforms.Compose([
        transforms.Resize((image_size * scale_factor, image_size * scale_factor)),  # Resize for downsampling
        BlurAndDownsample(sigma=1.0, scale_factor=scale_factor),
        transforms.Resize((image_size, image_size)),  # Resize back to desired LR size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset_dir = 'data'

    if not os.path.exists(dataset_dir):
        os.makedirs(dataset_dir)

    # Download and load the full dataset
    full_dataset = OxfordIIITPet(root='/data', download=True, transform=None)

    total_size = len(full_dataset)
    test_size = int(total_size * test_split)
    train_size = total_size - test_size
    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])

    limited_train_size = train_size
    limited_test_size = test_size

    train_dataset, _ = torch.utils.data.random_split(train_dataset,
                                                     [limited_train_size, train_size - limited_train_size])
    test_dataset, _ = torch.utils.data.random_split(test_dataset, [limited_test_size, test_size - limited_test_size])

    def lr_hr_transform(data):
        hr_image = hr_transform(data)
        lr_image = lr_transform(data)
        return lr_image, hr_image

    # Update dataset classes to use custom transform
    train_dataset = TransformDataset(train_dataset, lr_hr_transform)
    test_dataset = TransformDataset(test_dataset, lr_hr_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return train_loader, test_loader

"""class DIV2KDataset(Dataset):
    def __init__(self, root_dir, hr_transform=None, lr_transform=None):
        self.root_dir = root_dir
        self.hr_transform = hr_transform
        self.lr_transform = lr_transform
        self.image_filenames = [f for f in os.listdir(root_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.image_filenames[idx])
        hr_image = Image.open(img_name).convert('RGB')  # Load as PIL image

        if self.hr_transform:
            hr_image = self.hr_transform(hr_image)

        lr_image = hr_image.copy()  # Create a copy of the HR image for LR transformation
        if self.lr_transform:
            lr_image = self.lr_transform(lr_image)

        return lr_image, hr_image


# Update the data preparation function for DIV2K
def prepare_datasets_div2k(image_size=128, batch_size=32, scale_factor=2, test_split=0.2, limit_dataset_size=None):
    # High-Resolution Transform
    hr_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Low-Resolution Transform
    lr_transform = transforms.Compose([
        transforms.Resize((image_size * scale_factor, image_size * scale_factor)),  # Resize for downsampling
        BlurAndDownsample(sigma=1.0, scale_factor=scale_factor),
        transforms.Resize((image_size, image_size)),  # Resize back to desired LR size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset_dir = '/content/drive/MyDrive/data/DIV2K_valid_HR'  # Path to the extracted DIV2K dataset
    full_dataset = DIV2KDataset(root_dir=dataset_dir)

    total_size = len(full_dataset)
    test_size = int(total_size * test_split)
    train_size = total_size - test_size
    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])

    if limit_dataset_size:
        train_dataset, _ = torch.utils.data.random_split(train_dataset, [limit_dataset_size, train_size - limit_dataset_size])
        test_dataset, _ = torch.utils.data.random_split(test_dataset, [limit_dataset_size, test_size - limit_dataset_size])

    def lr_hr_transform(data):
        hr_image = hr_transform(data)
        lr_image = lr_transform(data)
        return lr_image, hr_image

    # Update dataset classes to use custom transform
    train_dataset = TransformDataset(train_dataset, lr_hr_transform)
    test_dataset = TransformDataset(test_dataset, lr_hr_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return train_loader, test_loader

# Load DIV2K dataset
train_loader, test_loader = prepare_datasets_div2k()"""

class TransformDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx][0]
        return self.transform(data)

train_loader, test_loader = prepare_datasets()

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # 64 -> 32
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32 -> 16
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 16 -> 8
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(64 * 16 * 16, 256)
        self.fc_logvar = nn.Linear(64 * 16 * 16, 256)

        # Decoder
        self.decoder_input = nn.Linear(256, 64 * 16 * 16)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # 8 -> 16
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # 16 -> 32
            nn.ReLU(),
            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),  # 32 -> 64
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = self.encoder(x)
        x = torch.flatten(x, start_dim=1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        z = self.reparameterize(mu, logvar)
        x = self.decoder_input(z)
        x = x.view(-1, 64, 16, 16)
        decoded = self.decoder(x)
        return decoded, mu, logvar


class PerceptualLoss(nn.Module):
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        vgg_features = models.vgg16(pretrained=True).features[:16].eval()
        self.feature_extractor = nn.Sequential(*vgg_features)
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, output, target):
        content_loss = F.mse_loss(self.feature_extractor(target), self.feature_extractor(output))
        return content_loss


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE().to(device)
perceptual_loss_fn = PerceptualLoss().to(device)


def save_tensor_images(images, epoch, batch):
    output_dir = '/content/drive/MyDrive/outputs'

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    file_path = f'{output_dir}/epoch_{epoch}_batch_{batch}.png'

    save_image(images.cpu(), file_path, nrow=4)
    print(f"Saved images to {file_path}")


def denormalize(tensor):
    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
    return tensor * std + mean


def loss_function(recon_x, x, mu, logvar):
    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')
    # Calculate KL divergence
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return 0.001 * MSE + KLD


models_dir = '/content/drive/MyDrive/models'

if not os.path.exists(models_dir):
    os.makedirs(models_dir)


epochs = 250


def train_and_validate(model, train_loader, test_loader, epochs):
    optimizer = optim.Adam(model.parameters(), lr=0.005)
    model.train()
    for epoch in range(epochs):
        for i, (lr_images, hr_images) in enumerate(train_loader):
            lr_images, hr_images = lr_images.to(device), hr_images.to(device)
            optimizer.zero_grad()
            sr_images, mu, logvar = model(lr_images)  # Generate super-resolved images and latent variables

            # Calculate losses
            recon_loss = loss_function(sr_images, hr_images, mu, logvar)
            perceptual_loss = perceptual_loss_fn(sr_images, hr_images)
            loss = recon_loss + 0.8 * perceptual_loss

            loss.backward()
            optimizer.step()
            if i % 100 == 0:
                print(f"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}")

            sr_images = denormalize(sr_images)
            if i % 500 == 0:
                # Save training progress images
                compare_images = torch.cat([hr_images[:4], sr_images[:4]])
                save_tensor_images(compare_images, epoch, i)  # Save images to disk

        # # Validation
        # model.eval()
        # with torch.no_grad():
        #     total_loss = 0
        #     count = 0
        #     for lr_images, hr_images in test_loader:
        #         lr_images, hr_images = lr_images.to(device), hr_images.to(device)
        #
        #         reconstruction, mu, logvar = model(lr_images)
        #
        #         # Calculate losses
        #         recon_loss = loss_function(reconstruction, hr_images, mu, logvar)
        #         perceptual_loss = perceptual_loss_fn(reconstruction, hr_images)
        #         loss = recon_loss + 50 * perceptual_loss
        #         total_loss += loss.item()
        #         count += 1
        #     avg_loss = total_loss / count
        #     print(f"Epoch: {epoch}, Validation Loss: {avg_loss}")

        # Save the model
        torch.save(model.state_dict(), f'/content/drive/MyDrive/models/vae_epoch_{epoch + 1}.pth')


def test(model, dataloader):
    model.eval()
    with torch.no_grad():
        for i, (images, _) in enumerate(dataloader):
            images = images.to(device)
            outputs, _, _ = model(images)
            if i == 0:
                compare_images(images, outputs)


def compare_images(original, reconstructed):
    if original.shape != reconstructed.shape:
        reconstructed = TF.resize(reconstructed, original.shape[-2:])
    figure, ax = plt.subplots(2, 10, figsize=(20, 4))
    for i in range(10):
        ax[0, i].imshow(original[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)
        ax[0, i].axis('off')
        ax[1, i].imshow(reconstructed[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)
        ax[1, i].axis('off')
    plt.show()


def calculate_psnr(model, dataloader):
    model.eval()
    total_psnr = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                psnr = compare_psnr(images[i].cpu().numpy(), outputs[i].cpu().numpy(), data_range=1)
                total_psnr += psnr
    average_psnr = total_psnr / len(dataloader.dataset)
    print(f'Average PSNR: {average_psnr} dB')


def calculate_ssim(model, dataloader):
    model.eval()
    total_ssim = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                img_np = (images[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                output_np = (outputs[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                ssim = compare_ssim(img_np, output_np, multichannel=True, win_size=3)
                total_ssim += ssim
    average_ssim = total_ssim / len(dataloader.dataset)
    print(f'Average SSIM: {average_ssim}')


train_and_validate(model, train_loader, test_loader, epochs)
test(model, test_loader)

def load_model(model_path):
    model = VAE()
    model.load_state_dict(torch.load(model_path))
    model.to(device) # Move the loaded model to the same device as the input data
    return model

model_path = f"/content/drive/MyDrive/models/vae_epoch_{epochs}.pth"
model = load_model(model_path)

calculate_psnr(model, test_loader)
calculate_ssim(model, test_loader)

import os
import gdown
import zipfile
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image, ImageFilter
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import OxfordIIITPet
from torchvision.transforms import functional as TF
from torchvision.utils import save_image
from skimage.metrics import structural_similarity as compare_ssim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model(model_path):
    model = VAE()
    model.load_state_dict(torch.load(model_path))
    model.to(device) # Move the loaded model to the same device as the input data
    return model

dataset_dir = '/content/drive/MyDrive/data/DIV2K_valid_HR'

model_path = f"/content/drive/MyDrive/models/vae_epoch_{epochs}.pth"
model = load_model(model_path)

class DIV2KDataset(Dataset):
    def __init__(self, root_dir, hr_transform=None, lr_transform=None):
        self.root_dir = root_dir
        self.hr_transform = hr_transform
        self.lr_transform = lr_transform
        self.image_filenames = [f for f in os.listdir(root_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.image_filenames[idx])
        hr_image = Image.open(img_name).convert('RGB')  # Load as PIL image

        if self.hr_transform:
            hr_image = self.hr_transform(hr_image)

        lr_image = hr_image.copy()  # Create a copy of the HR image for LR transformation
        if self.lr_transform:
            lr_image = self.lr_transform(lr_image)

        return lr_image, hr_image


# Update the data preparation function for DIV2K
def prepare_datasets_div2k(image_size=128, batch_size=32, scale_factor=2, test_split=0.2, limit_dataset_size=None):
    # High-Resolution Transform
    hr_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Low-Resolution Transform
    lr_transform = transforms.Compose([
        transforms.Resize((image_size * scale_factor, image_size * scale_factor)),  # Resize for downsampling
        BlurAndDownsample(sigma=1.0, scale_factor=scale_factor),
        transforms.Resize((image_size, image_size)),  # Resize back to desired LR size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset_dir = '/content/drive/MyDrive/data/DIV2K_valid_HR'  # Path to the extracted DIV2K dataset
    full_dataset = DIV2KDataset(root_dir=dataset_dir)

    total_size = len(full_dataset)
    test_size = int(total_size * test_split)
    train_size = total_size - test_size
    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])

    if limit_dataset_size:
        train_dataset, _ = torch.utils.data.random_split(train_dataset, [limit_dataset_size, train_size - limit_dataset_size])
        test_dataset, _ = torch.utils.data.random_split(test_dataset, [limit_dataset_size, test_size - limit_dataset_size])

    def lr_hr_transform(data):
        hr_image = hr_transform(data)
        lr_image = lr_transform(data)
        return lr_image, hr_image

    # Update dataset classes to use custom transform
    train_dataset = TransformDataset(train_dataset, lr_hr_transform)
    test_dataset = TransformDataset(test_dataset, lr_hr_transform)
    return train_loader, test_loader

# Load DIV2K dataset
train_loader, test_loader = prepare_datasets_div2k()

train_and_validate(model, train_loader, test_loader, epochs)
test(model, test_loader)

import os
import gdown
import zipfile
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image, ImageFilter
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import OxfordIIITPet
from torchvision.transforms import functional as TF
from torchvision.utils import save_image
from skimage.metrics import structural_similarity as compare_ssim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BlurAndDownsample(nn.Module):
    def __init__(self, sigma=1.0, scale_factor=2):
        super(BlurAndDownsample, self).__init__()
        self.sigma = sigma
        self.scale_factor = scale_factor

    def gaussian_blur(self, img):
        # Applying Gaussian blur
        img = img.filter(ImageFilter.GaussianBlur(self.sigma))
        return img

    def downsample(self, img):
        # Downsample using bicubic interpolation
        width, height = img.size
        new_size = (width // self.scale_factor, height // self.scale_factor)
        img = TF.resize(img, new_size, interpolation=transforms.InterpolationMode.BICUBIC)
        return img

    def forward(self, img):
        img = self.gaussian_blur(img)
        img = self.downsample(img)
        return img

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # 64 -> 32
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32 -> 16
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 16 -> 8
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(64 * 16 * 16, 256)
        self.fc_logvar = nn.Linear(64 * 16 * 16, 256)

        # Decoder
        self.decoder_input = nn.Linear(256, 64 * 16 * 16)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # 8 -> 16
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # 16 -> 32
            nn.ReLU(),
            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),  # 32 -> 64
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = self.encoder(x)
        x = torch.flatten(x, start_dim=1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        z = self.reparameterize(mu, logvar)
        x = self.decoder_input(z)
        x = x.view(-1, 64, 16, 16)
        decoded = self.decoder(x)
        return decoded, mu, logvar


class PerceptualLoss(nn.Module):
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        vgg_features = models.vgg16(pretrained=True).features[:16].eval()
        self.feature_extractor = nn.Sequential(*vgg_features)
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, output, target):
        content_loss = F.mse_loss(self.feature_extractor(target), self.feature_extractor(output))
        return content_loss

def load_model(model_path):
    model = VAE()
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.to(device) # Move the loaded model to the same device as the input data
    return model

# Define the DIV2KDataset for testing
class DIV2KDataset(Dataset):
    def __init__(self, root_dir, hr_transform=None, lr_transform=None):
        self.root_dir = root_dir
        self.hr_transform = hr_transform
        self.lr_transform = lr_transform
        self.image_filenames = [f for f in os.listdir(root_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.image_filenames[idx])
        hr_image = Image.open(img_name).convert('RGB')  # Load as PIL image

        if self.hr_transform:
            hr_image = self.hr_transform(hr_image)

        lr_image = hr_image.copy()  # Create a copy of the HR image for LR transformation
        if self.lr_transform:
            lr_image = self.lr_transform(lr_image)

        return lr_image, hr_image

import torch
import torchvision.transforms.functional as TF
import os
from torchvision.utils import save_image

def test(model, dataloader, save_path='/content/drive/MyDrive/reconstructed_images'):
    model.eval()
    os.makedirs(save_path, exist_ok=True)  # Create directory if it doesn't exist
    with torch.no_grad():
        for batch_idx, (images, _) in enumerate(dataloader):
            images = images.to(device)
            outputs, _, _ = model(images)
            save_images(images, outputs, save_path, batch_idx)

def save_images(original, reconstructed, save_path, batch_idx):
    if original.shape != reconstructed.shape:
        reconstructed = TF.resize(reconstructed, original.shape[-2:])

    original_images = original.cpu() * 0.5 + 0.5  # Unnormalize
    reconstructed_images = reconstructed.cpu() * 0.5 + 0.5  # Unnormalize

    for i in range(original_images.size(0)):
        original_img_path = os.path.join(save_path, f'original_batch{batch_idx}_img{i}.png')
        reconstructed_img_path = os.path.join(save_path, f'reconstructed_batch{batch_idx}_img{i}.png')

        save_image(original_images[i], original_img_path)
        save_image(reconstructed_images[i], reconstructed_img_path)

    print(f'Saved batch {batch_idx} images.')

def prepare_datasets_div2k(image_size=128, batch_size=32, scale_factor=2, limit_dataset_size=None):
    # High-Resolution Transform
    hr_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Low-Resolution Transform
    lr_transform = transforms.Compose([
        transforms.Resize((image_size * scale_factor, image_size * scale_factor)),  # Resize for downsampling
        BlurAndDownsample(sigma=1.0, scale_factor=scale_factor),
        transforms.Resize((image_size, image_size)),  # Resize back to desired LR size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset_dir = '/content/drive/MyDrive/data/DIV2K_valid_HR'  # Path to the extracted DIV2K dataset
    test_dataset = DIV2KDataset(root_dir=dataset_dir)

    def lr_hr_transform(data):
        hr_image = hr_transform(data)
        lr_image = lr_transform(data)
        return lr_image, hr_image

    # Update dataset classes to use custom transform
    test_dataset = TransformDataset(test_dataset, lr_hr_transform)

    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return test_loader

# Load DIV2K dataset
test_loader = prepare_datasets_div2k()

class TransformDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx][0]
        return self.transform(data)

def calculate_psnr(model, dataloader):
    model.eval()
    total_psnr = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                psnr = compare_psnr(images[i].cpu().numpy(), outputs[i].cpu().numpy(), data_range=1)
                total_psnr += psnr
    average_psnr = total_psnr / len(dataloader.dataset)
    print(f'Average PSNR: {average_psnr} dB')


def calculate_ssim(model, dataloader):
    model.eval()
    total_ssim = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                img_np = (images[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                output_np = (outputs[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                ssim = compare_ssim(img_np, output_np, multichannel=True, win_size=3)
                total_ssim += ssim
    average_ssim = total_ssim / len(dataloader.dataset)
    print(f'Average SSIM: {average_ssim}')

model_path = f"/content/drive/MyDrive/models/vae_epoch_250.pth"
model = load_model(model_path)

test(model, test_loader)
calculate_psnr(model, test_loader)
calculate_ssim(model, test_loader)

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.transforms import functional as TF
from torchvision.utils import save_image
from PIL import Image, ImageFilter
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
from skimage.metrics import structural_similarity as compare_ssim
import matplotlib.pyplot as plt
import numpy as np

# Define the Blur and Downsample module
class BlurAndDownsample:
    def __init__(self, sigma=1.0, scale_factor=2):
        self.sigma = sigma
        self.scale_factor = scale_factor

    def gaussian_blur(self, img):
        return img.filter(ImageFilter.GaussianBlur(self.sigma))

    def downsample(self, img):
        width, height = img.size
        new_size = (width // self.scale_factor, height // self.scale_factor)
        return TF.resize(img, new_size, interpolation=Image.BICUBIC)

    def __call__(self, img):
        img = self.gaussian_blur(img)
        img = self.downsample(img)
        return img

# Define the DIV2K Dataset
class DIV2KDataset(Dataset):
    def __init__(self, root_dir, hr_transform=None, lr_transform=None):
        self.root_dir = root_dir
        self.hr_transform = hr_transform
        self.lr_transform = lr_transform
        self.image_filenames = [f for f in os.listdir(root_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.image_filenames[idx])
        hr_image = Image.open(img_name).convert('RGB')  # Load as PIL image

        if self.hr_transform:
            hr_image = self.hr_transform(hr_image)

        lr_image = hr_image.copy()  # Create a copy of the HR image for LR transformation
        if self.lr_transform:
            lr_image = self.lr_transform(lr_image)

        return lr_image, hr_image

# Define the TransformDataset wrapper
class TransformDataset(Dataset):
    def __init__(self, dataset, transform):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        lr_image, hr_image = self.dataset[idx]
        return self.transform(lr_image, hr_image)

# Prepare the DIV2K dataset with TransformDataset
def prepare_datasets_div2k(image_size=128, batch_size=32, scale_factor=2, test_split=0.2):
    hr_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    lr_transform = transforms.Compose([
        transforms.Resize((image_size * scale_factor, image_size * scale_factor)),  # Upscale for downsampling
        BlurAndDownsample(sigma=1.0, scale_factor=scale_factor),  # Apply blur and downsample
        transforms.Resize((image_size, image_size)),  # Resize back to desired LR size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset_dir = 'path_to_your_div2k_images'
    full_dataset = DIV2KDataset(root_dir=dataset_dir)

    total_size = len(full_dataset)
    test_size = int(total_size * test_split)
    train_size = total_size - test_size
    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])

    def transform_pair(lr_image, hr_image):
        return lr_transform(lr_image), hr_transform(hr_image)

    # Wrap datasets in TransformDataset
    train_dataset = TransformDataset(train_dataset, transform_pair)
    test_dataset = TransformDataset(test_dataset, transform_pair)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return train_loader, test_loader

# VAE Model
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # 64 -> 32
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32 -> 16
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 16 -> 8
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(64 * 16 * 16, 256)
        self.fc_logvar = nn.Linear(64 * 16 * 16, 256)

        self.decoder_input = nn.Linear(256, 64 * 16 * 16)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # 8 -> 16
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # 16 -> 32
            nn.ReLU(),
            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),  # 32 -> 64
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = self.encoder(x)
        x = torch.flatten(x, start_dim=1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        z = self.reparameterize(mu, logvar)
        x = self.decoder_input(z)
        x = x.view(-1, 64, 16, 16)
        decoded = self.decoder(x)
        return decoded, mu, logvar

# Perceptual Loss
class PerceptualLoss(nn.Module):
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        vgg_features = models.vgg16(pretrained=True).features[:16].eval()
        self.feature_extractor = nn.Sequential(*vgg_features)
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, output, target):
        return F.mse_loss(self.feature_extractor(target), self.feature_extractor(output))

# Function to save tensor images
def save_tensor_images(images, epoch, batch):
    output_dir = './outputs'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    file_path = f'{output_dir}/epoch_{epoch}_batch_{batch}.png'
    save_image(images.cpu(), file_path, nrow=4)
    print(f"Saved images to {file_path}")

# Function to denormalize images
def denormalize(tensor):
    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
    return tensor * std + mean

# Loss function for the VAE
def loss_function(recon_x, x, mu, logvar):
    MSE = F.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return 0.001 * MSE + KLD

# Load pre-trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE().to(device)
model.load_state_dict(torch.load('path_to_pretrained_vae.pth'))  # Replace with your pre-trained model path

# Fine-tune the model
def train_and_validate(model, train_loader, test_loader, epochs=100):
    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    perceptual_loss_fn = PerceptualLoss().to(device)
    model.train()

    for epoch in range(epochs):
        for i, (lr_images, hr_images) in enumerate(train_loader):
            lr_images, hr_images = lr_images.to(device), hr_images.to(device)
            optimizer.zero_grad()
            sr_images, mu, logvar = model(lr_images)

            recon_loss = loss_function(sr_images, hr_images, mu, logvar)
            perceptual_loss = perceptual_loss_fn(sr_images, hr_images)
            loss = recon_loss + 0.8 * perceptual_loss

            loss.backward()
            optimizer.step()

            if i % 100 == 0:
                print(f"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}")

            sr_images = denormalize(sr_images)
            if i % 500 == 0:
                compare_images = torch.cat([hr_images[:4], sr_images[:4]])
                save_tensor_images(compare_images, epoch, i)

        # Save the fine-tuned model
        torch.save(model.state_dict(), f'./models/vae_finetuned_epoch_{epoch + 1}.pth')

import os
import gdown
import zipfile
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image, ImageFilter
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import OxfordIIITPet
from torchvision.transforms import functional as TF
from torchvision.utils import save_image
from skimage.metrics import structural_similarity as compare_ssim


class BlurAndDownsample(nn.Module):
    def __init__(self, sigma=1.0, scale_factor=2):
        super(BlurAndDownsample, self).__init__()
        self.sigma = sigma
        self.scale_factor = scale_factor

    def gaussian_blur(self, img):
        # Applying Gaussian blur
        img = img.filter(ImageFilter.GaussianBlur(self.sigma))
        return img

    def downsample(self, img):
        # Downsample using bicubic interpolation
        width, height = img.size
        new_size = (width // self.scale_factor, height // self.scale_factor)
        img = TF.resize(img, new_size, interpolation=transforms.InterpolationMode.BICUBIC)
        return img

    def forward(self, img):
        img = self.gaussian_blur(img)
        img = self.downsample(img)
        return img

class DIV2KDataset(Dataset):
    def __init__(self, root_dir, hr_transform=None, lr_transform=None):
        self.root_dir = root_dir
        self.hr_transform = hr_transform
        self.lr_transform = lr_transform
        self.image_filenames = [f for f in os.listdir(root_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.image_filenames[idx])
        hr_image = Image.open(img_name).convert('RGB')  # Load as PIL image

        if self.hr_transform:
            hr_image = self.hr_transform(hr_image)

        lr_image = hr_image.copy()  # Create a copy of the HR image for LR transformation
        if self.lr_transform:
            lr_image = self.lr_transform(lr_image)

        return lr_image, hr_image


# Update the data preparation function for DIV2K
def prepare_datasets_div2k(image_size=128, batch_size=32, scale_factor=2, test_split=0.2, limit_dataset_size=None):
    # High-Resolution Transform
    hr_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Low-Resolution Transform
    lr_transform = transforms.Compose([
        transforms.Resize((image_size * scale_factor, image_size * scale_factor)),  # Resize for downsampling
        BlurAndDownsample(sigma=1.0, scale_factor=scale_factor),
        transforms.Resize((image_size, image_size)),  # Resize back to desired LR size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset_dir = '/content/drive/MyDrive/DIV2K_train_HR'  # Path to the extracted DIV2K dataset
    full_dataset = DIV2KDataset(root_dir=dataset_dir)

    total_size = len(full_dataset)
    test_size = int(total_size * test_split)
    train_size = total_size - test_size
    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])

    if limit_dataset_size:
        train_dataset, _ = torch.utils.data.random_split(train_dataset, [limit_dataset_size, train_size - limit_dataset_size])
        test_dataset, _ = torch.utils.data.random_split(test_dataset, [limit_dataset_size, test_size - limit_dataset_size])

    def lr_hr_transform(data):
        hr_image = hr_transform(data)
        lr_image = lr_transform(data)
        return lr_image, hr_image

    # Update dataset classes to use custom transform
    train_dataset = TransformDataset(train_dataset, lr_hr_transform)
    test_dataset = TransformDataset(test_dataset, lr_hr_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return train_loader, test_loader

class TransformDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx][0]
        return self.transform(data)

# Load DIV2K dataset
train_loader, test_loader = prepare_datasets_div2k()

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # 64 -> 32
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32 -> 16
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 16 -> 8
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(64 * 16 * 16, 256)
        self.fc_logvar = nn.Linear(64 * 16 * 16, 256)

        # Decoder
        self.decoder_input = nn.Linear(256, 64 * 16 * 16)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # 8 -> 16
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # 16 -> 32
            nn.ReLU(),
            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),  # 32 -> 64
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = self.encoder(x)
        x = torch.flatten(x, start_dim=1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        z = self.reparameterize(mu, logvar)
        x = self.decoder_input(z)
        x = x.view(-1, 64, 16, 16)
        decoded = self.decoder(x)
        return decoded, mu, logvar


class PerceptualLoss(nn.Module):
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        vgg_features = models.vgg16(pretrained=True).features[:16].eval()
        self.feature_extractor = nn.Sequential(*vgg_features)
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, output, target):
        content_loss = F.mse_loss(self.feature_extractor(target), self.feature_extractor(output))
        return content_loss


# Load pre-trained model

model_path = f"/content/drive/MyDrive/models/vae_epoch_250.pth"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE()
model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))  # Replace with your pre-trained model path
model.to(device)
perceptual_loss_fn = PerceptualLoss().to(device)

t_epochs = 250

def save_tensor_images(images, epoch, batch):
    output_dir = '/content/drive/MyDrive/outputs'

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    file_path = f'{output_dir}/epoch_{t_epochs+epoch}_batch_{batch}.png'

    save_image(images.cpu(), file_path, nrow=4)
    print(f"Saved images to {file_path}")


def denormalize(tensor):
    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
    return tensor * std + mean


def loss_function(recon_x, x, mu, logvar):
    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')
    # Calculate KL divergence
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return 0.001 * MSE + KLD


models_dir = '/content/drive/MyDrive/models'

if not os.path.exists(models_dir):
    os.makedirs(models_dir)


def train_and_validate(model, train_loader, test_loader, epochs=20):
    optimizer = optim.Adam(model.parameters(), lr=0.005)
    model.train()
    for epoch in range(epochs):
        for i, (lr_images, hr_images) in enumerate(train_loader):
            lr_images, hr_images = lr_images.to(device), hr_images.to(device)
            optimizer.zero_grad()
            sr_images, mu, logvar = model(lr_images)  # Generate super-resolved images and latent variables

            # Calculate losses
            recon_loss = loss_function(sr_images, hr_images, mu, logvar)
            perceptual_loss = perceptual_loss_fn(sr_images, hr_images)
            loss = recon_loss + 0.8 * perceptual_loss

            loss.backward()
            optimizer.step()
            if i % 100 == 0:
                print(f"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}")

            sr_images = denormalize(sr_images)
            if i % 500 == 0:
                # Save training progress images
                compare_images = torch.cat([hr_images[:4], sr_images[:4]])
                save_tensor_images(compare_images, epoch, i)  # Save images to disk

        # # Validation
        # model.eval()
        # with torch.no_grad():
        #     total_loss = 0
        #     count = 0
        #     for lr_images, hr_images in test_loader:
        #         lr_images, hr_images = lr_images.to(device), hr_images.to(device)
        #
        #         reconstruction, mu, logvar = model(lr_images)
        #
        #         # Calculate losses
        #         recon_loss = loss_function(reconstruction, hr_images, mu, logvar)
        #         perceptual_loss = perceptual_loss_fn(reconstruction, hr_images)
        #         loss = recon_loss + 50 * perceptual_loss
        #         total_loss += loss.item()
        #         count += 1
        #     avg_loss = total_loss / count
        #     print(f"Epoch: {epoch}, Validation Loss: {avg_loss}")

        # Save the model
        torch.save(model.state_dict(), f'/content/drive/MyDrive/models/vae_epoch_{t_epochs + epoch + 1}.pth')


def test(model, dataloader, save_path='/content/drive/MyDrive/reconstructed_images'):
    model.eval()
    os.makedirs(save_path, exist_ok=True)  # Create directory if it doesn't exist
    with torch.no_grad():
        for batch_idx, (images, _) in enumerate(dataloader):
            images = images.to(device)
            outputs, _, _ = model(images)
            save_images(images, outputs, save_path, batch_idx)

def save_images(original, reconstructed, save_path, batch_idx):
    if original.shape != reconstructed.shape:
        reconstructed = TF.resize(reconstructed, original.shape[-2:])

    original_images = original.cpu() * 0.5 + 0.5  # Unnormalize
    reconstructed_images = reconstructed.cpu() * 0.5 + 0.5  # Unnormalize

    for i in range(original_images.size(0)):
        original_img_path = os.path.join(save_path, f'original_batch{batch_idx}_img{i}.png')
        reconstructed_img_path = os.path.join(save_path, f'reconstructed_batch{batch_idx}_img{i}.png')

        save_image(original_images[i], original_img_path)
        save_image(reconstructed_images[i], reconstructed_img_path)

    print(f'Saved batch {batch_idx} images.')


def compare_images(original, reconstructed):
    if original.shape != reconstructed.shape:
        reconstructed = TF.resize(reconstructed, original.shape[-2:])
    figure, ax = plt.subplots(2, 10, figsize=(20, 4))
    for i in range(10):
        ax[0, i].imshow(original[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)
        ax[0, i].axis('off')
        ax[1, i].imshow(reconstructed[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)
        ax[1, i].axis('off')
    plt.show()


def calculate_psnr(model, dataloader):
    model.eval()
    total_psnr = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                psnr = compare_psnr(images[i].cpu().numpy(), outputs[i].cpu().numpy(), data_range=1)
                total_psnr += psnr
    average_psnr = total_psnr / len(dataloader.dataset)
    print(f'Average PSNR: {average_psnr} dB')


def calculate_ssim(model, dataloader):
    model.eval()
    total_ssim = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                img_np = (images[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                output_np = (outputs[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                ssim = compare_ssim(img_np, output_np, multichannel=True, win_size=3)
                total_ssim += ssim
    average_ssim = total_ssim / len(dataloader.dataset)
    print(f'Average SSIM: {average_ssim}')

def calculate_psnr(model, dataloader):
    model.eval()
    total_psnr = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                psnr = compare_psnr(images[i].cpu().numpy(), outputs[i].cpu().numpy(), data_range=1)
                total_psnr += psnr
    average_psnr = total_psnr / len(dataloader.dataset)
    print(f'Average PSNR: {average_psnr} dB')


def calculate_ssim(model, dataloader):
    model.eval()
    total_ssim = 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs, _, _ = model(images)
            outputs = TF.resize(outputs, images.shape[-2:])
            for i in range(images.size(0)):
                img_np = (images[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                output_np = (outputs[i].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                ssim = compare_ssim(img_np, output_np, multichannel=True, win_size=3)
                total_ssim += ssim
    average_ssim = total_ssim / len(dataloader.dataset)
    print(f'Average SSIM: {average_ssim}')


# train_and_validate(model, train_loader, test_loader, epochs)
train_and_validate(model, train_loader, test_loader)
test(model, test_loader)
calculate_psnr(model, test_loader)
calculate_ssim(model, test_loader)

